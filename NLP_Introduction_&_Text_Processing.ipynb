{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Introduction & Text Processing |\n",
        "# Assignment"
      ],
      "metadata": {
        "id": "BuRNJEtVWY0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) What is Computational Linguistics and how does it relate to NLP?\n",
        "\n",
        "Ans: Computational Linguistics (CL) is a field that focuses on using computers to understand, interpret, analyze, and generate human language. It combines knowledge from linguistics, computer science, mathematics, and artificial intelligence.\n",
        "\n",
        "Computational linguistics tries to answer questions like:\n",
        "\n",
        "- How do humans understand language?\n",
        "\n",
        "- How can computers be programmed to understand and process language?\n",
        "\n",
        "- How can language rules be modeled mathematically?\n",
        "\n",
        "How Computational Linguistics Relates to NLP\n",
        "\n",
        "Natural Language Processing (NLP) is a practical application area of computational linguistics.\n",
        "While computational linguistics is the theoretical and scientific study, NLP focuses on building working systems such as:\n",
        "\n",
        "- Speech recognition (Alexa, Siri)\n",
        "\n",
        "- Machine translation (Google Translate)\n",
        "\n",
        "- Chatbots (like ChatGPT)\n",
        "\n",
        "- Sentiment analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "maNazQJpWwID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2) Briefly describe the historical evolution of Natural Language Processing.\n",
        "\n",
        "Ans:\n",
        "Historical Evolution of Natural Language Processing (NLP)\n",
        "The evolution of NLP can be divided into major phases:\n",
        "\n",
        "- 1950s – Early Rule-Based Approaches\n",
        "NLP began with simple rule-based systems and machine translation attempts. Alan Turing introduced the Turing Test (1950).\n",
        "\n",
        "- 1960s–1980s – Linguistic and Symbolic Models\n",
        "Systems used grammar rules to understand language. Famous programs like ELIZA (1966) and SHRDLU appeared.\n",
        "\n",
        "- 1980s–1990s – Statistical NLP\n",
        "The shift from rules to probability-based models happened due to the availability of large datasets. Methods like Hidden Markov Models and n-grams became popular.\n",
        "\n",
        "- 2000s – Machine Learning Era\n",
        "NLP started using ML algorithms such as SVMs, Decision Trees, and Logistic Regression for tasks like text classification and speech recognition.\n",
        "\n",
        "- 2010s–Present – Deep Learning and Neural NLP\n",
        "With neural networks and models like RNNs, LSTMs, Transformers (BERT, GPT), NLP achieved human-like language understanding and generation."
      ],
      "metadata": {
        "id": "ouRYRxIfXqwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (3) List and explain three major use cases of NLP in today’s tech industry.\n",
        "\n",
        "Ans:\n",
        "- Machine Translation\n",
        "\n",
        "NLP is used in automated translation systems like Google Translate or Microsoft Translator.\n",
        "These systems convert text or speech from one language to another by understanding grammar, meaning, and context.\n",
        "\n",
        "- Sentiment Analysis\n",
        "\n",
        "Companies use NLP to analyze customer feedback, reviews, or social media posts.\n",
        "It helps determine whether the expressed opinion is positive, negative, or neutral, which supports brand monitoring, customer service, and market analysis.\n",
        "\n",
        "- Chatbots and Virtual Assistants\n",
        "\n",
        "NLP powers conversational systems such as ChatGPT, Siri, Alexa, and WhatsApp chatbots.\n",
        "These systems understand user queries and generate meaningful responses, helping in customer support, automation, and user interaction."
      ],
      "metadata": {
        "id": "nvoKDY2oY058"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (4) What is text normalization and why is it essential in text processing tasks?\n",
        "\n",
        "Ans:\n",
        "- Text Normalization\n",
        "\n",
        "Text normalization is the process of converting text into a standard and consistent format before processing it in NLP tasks. It reduces variations in language so that similar words are treated the same by algorithms.\n",
        "\n",
        "Normalization steps may include:\n",
        "- Lowercasing text\n",
        "\n",
        "- Removing punctuation or special characters\n",
        "\n",
        "- Expanding contractions (e.g., don’t → do not)\n",
        "\n",
        "- Lemmatization or stemming (e.g., running → run)"
      ],
      "metadata": {
        "id": "KT3MAo7eZc0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (5) Compare and contrast stemming and lemmatization with suitable examples ?\n",
        "\n",
        "Ans:\n",
        "- Stemming\n",
        "\n",
        "Stemming is a technique used to remove word endings and reduce a word to its root form.\n",
        "\n",
        "The resulting root may not be a meaningful word.\n",
        "\n",
        "It is rule-based and faster but less accurate.\n",
        "\n",
        "Example: Running → runn, Studies → studi.\n",
        "\n",
        "- Lemmatization\n",
        "\n",
        "Lemmatization reduces a word to its meaningful base form, called a lemma.\n",
        "\n",
        "The output is always a valid dictionary word.\n",
        "\n",
        "It uses linguistic knowledge like grammar and vocabulary, making it more accurate but slower.\n",
        "\n",
        "Example: Running → run, Better → good."
      ],
      "metadata": {
        "id": "zT8YFtE3mnGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6\n",
        "import re\n",
        "\n",
        "text = \"\"\"\n",
        "Hello team, please contact us at support@xyz.com for technical issues,\n",
        "or reach out to our HR at hr@xyz.com. You can also connect with John at\n",
        "john.doe@xyz.org and jenny via jenny_clarke126@mail.co.us.\n",
        "For partnership inquiries, email partners@xyz.biz.\n",
        "\"\"\"\n",
        "\n",
        "# Regex pattern for extracting emails\n",
        "pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "# Extract emails\n",
        "emails = re.findall(pattern, text)\n",
        "\n",
        "# Display output\n",
        "print(\"Extracted Email Addresses:\")\n",
        "for email in emails:\n",
        "    print(email)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UepOpBOrnlRx",
        "outputId": "ea35c017-ecda-4380-f883-98e3b29fdc72"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Email Addresses:\n",
            "support@xyz.com\n",
            "hr@xyz.com\n",
            "john.doe@xyz.org\n",
            "jenny_clarke126@mail.co.us\n",
            "partners@xyz.biz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Sample paragraph\n",
        "text = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\n",
        "\"\"\"\n",
        "\n",
        "# Download tokenizer (only needed once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenizing the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Frequency Distribution\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "# Displaying output\n",
        "print(\"Tokenized Words:\\n\", tokens)\n",
        "print(\"\\nTop 10 Most Common Words:\")\n",
        "for word, freq in freq_dist.most_common(10):\n",
        "    print(f\"{word}: {freq}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a21Lv35cnx01",
        "outputId": "8697573c-a46d-44fb-e22a-7d32db7e0a48"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Words:\n",
            " ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'As', 'technology', 'advances', ',', 'the', 'role', 'of', 'NLP', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
            "\n",
            "Top 10 Most Common Words:\n",
            ",: 7\n",
            ".: 4\n",
            "NLP: 3\n",
            "and: 3\n",
            "is: 2\n",
            "of: 2\n",
            "Natural: 1\n",
            "Language: 1\n",
            "Processing: 1\n",
            "(: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8\n",
        "!pip install spacy\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load English spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"\"\"\n",
        "Apple is working with Microsoft and Google to build AI-powered applications.\n",
        "Elon Musk also announced updates at Tesla headquarters in California.\n",
        "\"\"\"\n",
        "\n",
        "# Process text\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Proper Nouns Found:\\n\")\n",
        "\n",
        "for token in doc:\n",
        "    if token.pos_ == \"PROPN\":  # PROPN = Proper Noun in spaCy\n",
        "        print(f\"{token.text} → {token.pos_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwDWulSRn49L",
        "outputId": "a6b97e3e-dbbd-423b-9cb6-09d4143a17b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Proper Nouns Found:\n",
            "\n",
            "Apple → PROPN\n",
            "Microsoft → PROPN\n",
            "Google → PROPN\n",
            "AI → PROPN\n",
            "Elon → PROPN\n",
            "Musk → PROPN\n",
            "Tesla → PROPN\n",
            "California → PROPN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9\n",
        "# Install gensim if not already installed\n",
        "!pip install gensim\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Download tokenizer resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Given dataset\n",
        "dataset = [\n",
        " \"Natural language processing enables computers to understand human language\",\n",
        " \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        " \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        " \"Text preprocessing is a critical step before training word embeddings\",\n",
        " \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "# Step 1: Tokenization and preprocessing\n",
        "processed_data = []\n",
        "\n",
        "for sentence in dataset:\n",
        "    tokens = word_tokenize(sentence.lower())  # lowercase + tokenize\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]  # remove punctuation\n",
        "    processed_data.append(tokens)\n",
        "\n",
        "print(\"Tokenized and Preprocessed Text:\")\n",
        "print(processed_data)\n",
        "\n",
        "# Step 2: Train Word2Vec model\n",
        "model = Word2Vec(sentences=processed_data, vector_size=50, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Step 3: Display similar words\n",
        "print(\"\\nWords similar to 'word':\")\n",
        "print(model.wv.most_similar(\"word\"))\n",
        "\n",
        "print(\"\\nWords similar to 'nlp':\")\n",
        "print(model.wv.most_similar(\"nlp\"))\n",
        "\n",
        "# Step 4: Show word vector example\n",
        "print(\"\\nVector representation for the word 'language':\")\n",
        "print(model.wv['language'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_KmCXYKoOOL",
        "outputId": "636e6e57-3835-4d7f-fdce-75249040abc4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Tokenized and Preprocessed Text:\n",
            "[['natural', 'language', 'processing', 'enables', 'computers', 'to', 'understand', 'human', 'language'], ['word', 'embeddings', 'are', 'a', 'type', 'of', 'word', 'representation', 'that', 'allows', 'words', 'with', 'similar', 'meaning', 'to', 'have', 'similar', 'representation'], ['word2vec', 'is', 'a', 'popular', 'word', 'embedding', 'technique', 'used', 'in', 'many', 'nlp', 'applications'], ['text', 'preprocessing', 'is', 'a', 'critical', 'step', 'before', 'training', 'word', 'embeddings'], ['tokenization', 'and', 'normalization', 'help', 'clean', 'raw', 'text', 'for', 'modeling']]\n",
            "\n",
            "Words similar to 'word':\n",
            "[('before', 0.2706666588783264), ('enables', 0.2547191083431244), ('meaning', 0.24074727296829224), ('normalization', 0.21101471781730652), ('nlp', 0.18646620213985443), ('are', 0.17563006281852722), ('raw', 0.16719907522201538), ('applications', 0.16099633276462555), ('help', 0.15025003254413605), ('popular', 0.1453729271888733)]\n",
            "\n",
            "Words similar to 'nlp':\n",
            "[('normalization', 0.395441472530365), ('raw', 0.3064466714859009), ('processing', 0.24199426174163818), ('representation', 0.20524880290031433), ('understand', 0.19091033935546875), ('word', 0.18646621704101562), ('of', 0.18231548368930817), ('text', 0.14431104063987732), ('clean', 0.14284271001815796), ('many', 0.1349509209394455)]\n",
            "\n",
            "Vector representation for the word 'language':\n",
            "[-0.01427243  0.00247354 -0.01435346 -0.00446246  0.00743265  0.01165815\n",
            "  0.00239335  0.00420327 -0.00823756  0.01443403 -0.0126253   0.00928865\n",
            " -0.01642479  0.00409265 -0.00994694 -0.00848883 -0.00622968  0.01131842\n",
            "  0.01159546 -0.00995739  0.00155119 -0.01697629  0.01560488  0.01851739\n",
            " -0.00547892  0.0016098   0.00148542  0.0109709  -0.01721632  0.00117946\n",
            "  0.01373915  0.00446265  0.00224046 -0.01865095  0.01696202 -0.01251354\n",
            " -0.00598015  0.00698242 -0.00156369  0.00282798  0.00357739 -0.01366643\n",
            " -0.01946715  0.01808809  0.01239627 -0.01383919  0.00679678  0.0004092\n",
            "  0.00951897 -0.01424249]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10\n",
        "\n",
        "As a data scientist at a fintech startup analyzing customer feedback, the goal is to convert thousands of raw customer reviews into meaningful insights. The NLP workflow would include the following major steps:\n",
        "\n",
        "- step 1: Data Collection\n",
        "Collect reviews from sources like CSV files, app reviews, surveys, or databases.\n",
        "\n",
        "- Step 2: Text Cleaning\n",
        "\n",
        "Remove noise such as:\n",
        "\n",
        "Special characters\n",
        "\n",
        "URLs\n",
        "\n",
        "Numbers\n",
        "\n",
        "Extra whitespace\n",
        "\n",
        "- Step 3: Text Preprocessing\n",
        "\n",
        "Apply normalization steps:\n",
        "\n",
        "Lowercasing\n",
        "\n",
        "Tokenization\n",
        "\n",
        "Stopword removal\n",
        "\n",
        "Lemmatization or stemming\n",
        "\n",
        "- Step 4: Exploratory Text Analysis\n",
        "\n",
        "Use NLP methods like:\n",
        "\n",
        "Frequency distribution (common words)\n",
        "\n",
        "N-grams (phrases like \"bad service\", “fraud alert”)\n",
        "\n",
        "- Step 5: Sentiment Analysis\n",
        "\n",
        "Use models like VADER, BERT, or TextBlob to classify feedback into positive, neutral, or negative sentiment.\n",
        "\n",
        "- Step 6: Topic Modeling\n",
        "\n",
        "Use LDA (Latent Dirichlet Allocation) to identify key themes such as:\n",
        "\n",
        "Loan approval issues\n",
        "\n",
        "Payment failures\n",
        "\n",
        "Customer support problems\n",
        "\n",
        "- Step 7: Visualization\n",
        "\n",
        "Create visual insights like:\n",
        "\n",
        "Word clouds\n",
        "\n",
        "Sentiment bar charts\n",
        "\n",
        "Topic clusters\n",
        "\n",
        "- Step 8: Reporting Insights\n",
        "\n",
        "Provide actionable recommendations to the business based on patterns found in sentiment and topics."
      ],
      "metadata": {
        "id": "bJSjIfqhoVFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install nltk spacy gensim wordcloud textblob\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "from gensim import corpora, models\n",
        "\n",
        "# Download resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample dataset (representative – normally thousands of reviews)\n",
        "reviews = [\n",
        "    \"The loan approval process was fast and easy!\",\n",
        "    \"Customer support is terrible, no one responds.\",\n",
        "    \"Great user experience, smooth transactions.\",\n",
        "    \"Payment failed twice and app kept crashing.\",\n",
        "    \"Love the interface, very intuitive and helpful.\"\n",
        "]\n",
        "\n",
        "# Step 1: Cleaning function\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # remove special chars\n",
        "    return text.lower()\n",
        "\n",
        "cleaned_reviews = [clean_text(review) for review in reviews]\n",
        "\n",
        "# Step 2: Tokenization & Stopword removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokenized = [[word for word in word_tokenize(review) if word not in stop_words] for review in cleaned_reviews]\n",
        "\n",
        "print(\"Tokenized and Cleaned Reviews:\\n\", tokenized)\n",
        "\n",
        "# Step 3: Sentiment Analysis\n",
        "sentiments = [TextBlob(review).sentiment.polarity for review in reviews]\n",
        "print(\"\\nSentiment Scores:\\n\", sentiments)\n",
        "\n",
        "# Step 4: Topic Modeling with LDA\n",
        "dictionary = corpora.Dictionary(tokenized)\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized]\n",
        "lda_model = models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=10)\n",
        "\n",
        "print(\"\\nIdentified Topics:\")\n",
        "for topic in lda_model.print_topics():\n",
        "    print(topic)\n",
        "\n",
        "# Step 5: Word Cloud\n",
        "text_combined = \" \".join(cleaned_reviews)\n",
        "wordcloud = WordCloud(width=600, height=400).generate(text_combined)\n",
        "\n",
        "wordcloud\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeV4LpuapS5u",
        "outputId": "7f5de99b-58a2-4a08-f592-4158a8972f80"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.12/dist-packages (1.9.4)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.12/dist-packages (0.19.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from wordcloud) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from wordcloud) (3.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized and Cleaned Reviews:\n",
            " [['loan', 'approval', 'process', 'fast', 'easy'], ['customer', 'support', 'terrible', 'one', 'responds'], ['great', 'user', 'experience', 'smooth', 'transactions'], ['payment', 'failed', 'twice', 'app', 'kept', 'crashing'], ['love', 'interface', 'intuitive', 'helpful']]\n",
            "\n",
            "Sentiment Scores:\n",
            " [0.37083333333333335, -1.0, 0.6000000000000001, -0.5, 0.35]\n",
            "\n",
            "Identified Topics:\n",
            "(0, '0.057*\"experience\" + 0.057*\"user\" + 0.057*\"transactions\" + 0.056*\"great\" + 0.056*\"smooth\" + 0.056*\"helpful\" + 0.056*\"love\" + 0.056*\"interface\" + 0.056*\"intuitive\" + 0.056*\"customer\"')\n",
            "(1, '0.063*\"kept\" + 0.063*\"payment\" + 0.063*\"twice\" + 0.063*\"app\" + 0.063*\"failed\" + 0.063*\"crashing\" + 0.063*\"loan\" + 0.063*\"fast\" + 0.063*\"approval\" + 0.063*\"process\"')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wordcloud.wordcloud.WordCloud at 0x783748f54ef0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}